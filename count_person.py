from ultralytics import YOLO
from PyQt5 import QtCore, QtWidgets, QtGui
import cv2
import os
import time
from threading import Thread
import sys

from openvino.runtime import Core
from PIL import Image
import numpy as np
from api import api1
import qss
from load import load_model2 as load_model
# ä¸ç„¶æ¯æ¬¡YOLOå¤„ç†éƒ½ä¼šè¾“å‡ºè°ƒè¯•ä¿¡æ¯
os.environ['YOLO_VERBOSE'] = 'False'


class Person_detect(QtWidgets.QMainWindow):

    def __init__(self):

        super().__init__()

        # è®¾ç½®ç•Œé¢
        self.setupUI()
        self.w = 768
        self.h = 576
        self.iscamera = True
        self.videoBtn.clicked.connect(self.startVideo)
        self.camBtn.clicked.connect(self.startCamera)
        self.imgBtn.clicked.connect(self.img_detect)
        self.stopBtn.clicked.connect(self.stop)

        # å®šä¹‰å®šæ—¶å™¨ï¼Œç”¨äºæ§åˆ¶æ˜¾ç¤ºè§†é¢‘çš„å¸§ç‡
        self.timer_camera = QtCore.QTimer()
        # å®šæ—¶åˆ°äº†ï¼Œå›è°ƒ self.show_camera
        self.timer_camera.timeout.connect(self.show_camera)

        # åŠ è½½ YOLO nano æ¨¡å‹ï¼Œç¬¬ä¸€æ¬¡æ¯”è¾ƒè€—æ—¶ï¼Œè¦20ç§’å·¦å³
        # self.model = YOLO('yolov8n.pt')
        # ------------------------------------------------------
        # self.model = 'xml_models/yolov8s_nncf_int8.xml'
        # self.core = Core()
        # self.det_ov_model = self.core.read_model(self.model)
        # self.device = 'GPU'
        # self.det_compiled_model = self.core.compile_model(
        #     self.det_ov_model, self.device)
        # self.label_map = ['person']
        # ---------------------å•ä¾‹æ¨¡å¼--------------------------#
        self.det_compiled_model = load_model.det_compiled_model
        self.label_map = load_model.label_map
        # ------------------------------------------------------
        # è¦å¤„ç†çš„è§†é¢‘å¸§å›¾ç‰‡é˜Ÿåˆ—ï¼Œç›®å‰å°±æ”¾1å¸§å›¾ç‰‡
        self.frameToAnalyze = []

        # å¯åŠ¨å¤„ç†è§†é¢‘å¸§ç‹¬ç«‹çº¿ç¨‹
        Thread(target=self.frameAnalyzeThreadFunc, daemon=True).start()

    def resizeEvent(self, event):
        # è·å–æ–°çš„çª—å£å¤§å°
        new_size = event.size()
        # æ›´æ–°æ ‡ç­¾çš„æ–‡æœ¬å†…å®¹
        print(f"Window size: {new_size.width()} x {new_size.height()}")
        self.w = new_size.width()
        self.h = new_size.height()

    def setupUI(self):

        self.resize(1250, 750)

        self.setWindowTitle('äººä½“è®¡æ•°')

        # central Widget
        centralWidget = QtWidgets.QWidget(self)
        self.setCentralWidget(centralWidget)
        centralWidget.setStyleSheet(
            qss.qss_background)
        # central Widget é‡Œé¢çš„ ä¸» layout
        mainLayout = QtWidgets.QVBoxLayout(centralWidget)

        # ç•Œé¢çš„ä¸ŠåŠéƒ¨åˆ† : å›¾å½¢å±•ç¤ºéƒ¨åˆ†
        topLayout = QtWidgets.QHBoxLayout()
        self.label_treated = QtWidgets.QLabel("è¯·é€‰æ‹©è¾“å…¥æº")
        self.label_treated.setAlignment(QtCore.Qt.AlignCenter)
        self.label_treated.setMinimumSize(768, 576)
        self.label_treated.setStyleSheet(
            'border:1px solid #D7E2F9;color:white;font-size:30px;')

        # topLayout.addWidget(self.label_ori_video)
        topLayout.addWidget(self.label_treated)

        mainLayout.addLayout(topLayout)

        # ç•Œé¢ä¸‹åŠéƒ¨åˆ†ï¼š è¾“å‡ºæ¡† å’Œ æŒ‰é’®
        groupBox = QtWidgets.QGroupBox(self)

        bottomLayout = QtWidgets.QHBoxLayout(groupBox)
        self.textLog = QtWidgets.QTextBrowser()
        bottomLayout.addWidget(self.textLog)

        mainLayout.addWidget(groupBox)

        btnLayout = QtWidgets.QVBoxLayout()
        self.videoBtn = QtWidgets.QPushButton('ğŸï¸è§†é¢‘æ–‡ä»¶')
        self.camBtn = QtWidgets.QPushButton('ğŸ“¹æ‘„åƒå¤´')
        self.imgBtn = QtWidgets.QPushButton('ğŸ†•å›¾ç‰‡')
        self.stopBtn = QtWidgets.QPushButton('ğŸ›‘åœæ­¢')
        self.videoBtn.setStyleSheet(qss.qss_btn)
        self.camBtn.setStyleSheet(qss.qss_btn)
        self.imgBtn.setStyleSheet(qss.qss_btn)
        self.stopBtn.setStyleSheet(qss.qss_btn)
        btnLayout.addWidget(self.videoBtn)
        btnLayout.addWidget(self.camBtn)
        btnLayout.addWidget(self.imgBtn)
        btnLayout.addWidget(self.stopBtn)

        bottomLayout.addLayout(btnLayout)

    def startCamera(self):

        # å‚è€ƒ https://docs.opencv.org/3.4/dd/d43/tutorial_py_video_display.html

        # åœ¨ windowsä¸ŠæŒ‡å®šä½¿ç”¨ cv2.CAP_DSHOW ä¼šè®©æ‰“å¼€æ‘„åƒå¤´å¿«å¾ˆå¤šï¼Œ
        self.cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
        self.iscamera = True
        # self.cap.set(3, 768)
        # self.cap.set(4, 576)
        if not self.cap.isOpened():
            print("0å·æ‘„åƒå¤´ä¸èƒ½æ‰“å¼€")
            return ()

        if self.timer_camera.isActive() == False:  # è‹¥å®šæ—¶å™¨æœªå¯åŠ¨
            self.timer_camera.start(50)

    def is_video_file(self, file_path):
        VIDEO_EXTENSIONS = ['.mp4', '.avi', '.mov', '.wmv', '.flv']
        extension = os.path.splitext(file_path)[-1].lower()
        return extension in VIDEO_EXTENSIONS

    def startVideo(self):
        file_path, _ = QtWidgets.QFileDialog.getOpenFileName(
            self, "é€‰æ‹©æ–‡ä»¶", "", "All Files (*)")
        if file_path:
            print(file_path)
        if self.is_video_file(file_path):
            self.cap = cv2.VideoCapture(file_path)
            self.iscamera = False
            if not self.cap.isOpened():
                print("è§†é¢‘æ–‡ä»¶ä¸èƒ½æ‰“å¼€")
                return ()

            if self.timer_camera.isActive() == False:  # è‹¥å®šæ—¶å™¨æœªå¯åŠ¨
                self.timer_camera.start(50)

    def show_camera(self):

        ret, frame = self.cap.read()  # ä»è§†é¢‘æµä¸­è¯»å–
        if not ret:
            return

        # æŠŠè¯»åˆ°çš„16:10å¸§çš„å¤§å°é‡æ–°è®¾ç½®
        if self.iscamera:
            frame = cv2.resize(frame, (768, 576))
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        else:
            w = frame.shape[1]
            h = frame.shape[0]
            if w > h:
                h = int(h*768/w)
                w = 768
            else:
                w = int(w*576/h)
                h = 576
            frame = cv2.resize(frame, (w, h))
            # è§†é¢‘è‰²å½©è½¬æ¢å›RGBï¼ŒOpenCV images as BGR
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        # qImage = QtGui.QImage(frame.data, frame.shape[1], frame.shape[0],
        #                       QtGui.QImage.Format_RGB888)  # å˜æˆQImageå½¢å¼
        # å¾€æ˜¾ç¤ºè§†é¢‘çš„Labelé‡Œ æ˜¾ç¤ºQImage
        # self.label_ori_video.setPixmap(QtGui.QPixmap.fromImage(qImage))

        # å¦‚æœå½“å‰æ²¡æœ‰å¤„ç†ä»»åŠ¡
        if not self.frameToAnalyze:
            self.frameToAnalyze.append(frame)

    def frameAnalyzeThreadFunc(self):

        while True:
            if not self.frameToAnalyze:
                time.sleep(0.01)
                continue

            frame = self.frameToAnalyze.pop(0)
            t1 = time.time()
            detections = api1.detect(
                frame, self.det_compiled_model, 1)[0]
            image_with_boxes, count = api1.draw_results(
                detections, frame, self.label_map)
            t2 = time.time()
            ms = int((t2-t1)*1000)
            cv2.putText(
                image_with_boxes, f'FPS:{int(1000/ms)}', (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 1)
            cv2.putText(
                image_with_boxes, f'Person:{count}', (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 255), 2)
            # img = cv2.resize(image_with_boxes, (768, 576))
            img = image_with_boxes
            img = cv2.cvtColor(img, cv2.COLOR_RGB2RGBA)
# ----------------------------------------------------
            # self.textLog.append(str(count))
            qImage = QtGui.QImage(img.data, img.shape[1], img.shape[0],
                                  QtGui.QImage.Format_RGBA8888)  # å˜æˆQImageå½¢å¼

            self.label_treated.setPixmap(
                QtGui.QPixmap.fromImage(qImage))  # å¾€æ˜¾ç¤ºLabelé‡Œ æ˜¾ç¤ºQImage
            # print(self.width(), self.height())
            time.sleep(0.01)

    def is_image_file(self, file_path):
        IMAGE_EXTENSIONS = ['.jpg', '.png']
        extension = os.path.splitext(file_path)[-1].lower()
        return extension in IMAGE_EXTENSIONS

    def img_detect(self):
        file_path, _ = QtWidgets.QFileDialog.getOpenFileName(
            self, "é€‰æ‹©æ–‡ä»¶", "", "All Files (*)")
        if file_path:
            print(file_path)
        if self.is_image_file(file_path):
            img = cv2.imread(file_path)
            w = img.shape[1]
            h = img.shape[0]
            if w > h:
                h = int(h*768/w)
                w = 768
            else:
                w = int(w*576/h)
                h = 576
            img = cv2.resize(img, (w, h))
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            detections = api1.detect(
                img, self.det_compiled_model, 1)[0]
            image_with_boxes, count = api1.draw_results(
                detections, img, self.label_map)
            cv2.putText(
                image_with_boxes, f'Person:{count}', (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 255), 2)

            img = image_with_boxes
            img = cv2.cvtColor(img, cv2.COLOR_RGB2RGBA)
            qImage = QtGui.QImage(img.data, img.shape[1], img.shape[0],
                                  QtGui.QImage.Format_RGBA8888)

            self.label_treated.setPixmap(
                QtGui.QPixmap.fromImage(qImage))

    def stop(self):
        self.timer_camera.stop()  # å…³é—­å®šæ—¶å™¨
        self.cap.release()  # é‡Šæ”¾è§†é¢‘æµ
        self.label_treated.clear()  # æ¸…ç©ºè§†é¢‘æ˜¾ç¤ºåŒºåŸŸ
        self.label_treated.setText('è¯·é€‰æ‹©è¾“å…¥æº')


if __name__ == '__main__':
    app = QtWidgets.QApplication(sys.argv)
    window = Person_detect()
    window.show()
    app.exec()
